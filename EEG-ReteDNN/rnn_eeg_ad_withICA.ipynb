{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34328,"status":"ok","timestamp":1699027803371,"user":{"displayName":"Alessia Conti","userId":"02978993272620717539"},"user_tz":-60},"id":"Ht7LvYaLpQTX","outputId":"cb499364-850d-451e-ab1d-7f903c37d3b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Collegamento a Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"Ht7LvYaLpQTX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad122e04"},"outputs":[],"source":["# Commented out IPython magic to ensure Python compatibility.\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Softmax, Dropout, Bidirectional\n","import scipy.io  # to load/save MAT files\n","import time\n","import datetime\n","from matplotlib import pyplot as plt\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import sklearn.decomposition\n","import sklearn.model_selection\n","import sys\n","import os\n","%matplotlib inline\n","%load_ext tensorboard"],"id":"ad122e04"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0c31bcc"},"outputs":[],"source":["tf.random.set_seed(42)\n","np.random.seed(42)"],"id":"c0c31bcc"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":441,"status":"ok","timestamp":1698914743820,"user":{"displayName":"Alessia Conti","userId":"02978993272620717539"},"user_tz":-60},"id":"75ca00d8","outputId":"bad6a765-9320-469f-e72f-6a8cc7525c0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow: 2.14.0\n","GPU device: \n"]}],"source":["#print('Tf Keras:', keras.__version__)\n","print('TensorFlow:', tf.__version__)\n","print('GPU device:', tf.test.gpu_device_name())"],"id":"75ca00d8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc02d425"},"outputs":[],"source":["if 'google.colab' in sys.modules:  # try to detect if we're running in colab or locally\n","  working_dir = '/content/drive/MyDrive/eeg_DNN-Conti/'   # DA MODIFICARE IN BASE AL PATH DEL PROPRIO DRIVE\n","  %cp '/content/drive/MyDrive/eeg_DNN-Conti/r_pca.py' .\n","else:\n","  working_dir = '.'"],"id":"fc02d425"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YPmJs0N3MhFj"},"outputs":[],"source":["print(working_dir)"],"id":"YPmJs0N3MhFj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9484af1e"},"outputs":[],"source":["import r_pca\n","#import mspca\n","multiscale_pca = False  # Compute MSPCA before PCA"],"id":"9484af1e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ee0803e0"},"outputs":[],"source":["log_dir_base = working_dir + '/logs/fit'\n","num_classes = 2"],"id":"ee0803e0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzuu_kMGIMuA"},"outputs":[],"source":["### SOLUZIONE 1: caricare dati + applicare ICA + salvare in cartella 'eeg2_ica'\n"," # (che poi verrà caricata in create_dataset alla riga dataset_dir = working_dir + '/eeg2_ica')\n"," # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html"],"id":"mzuu_kMGIMuA"},{"cell_type":"code","execution_count":null,"metadata":{"id":"20c9e350"},"outputs":[],"source":["def create_dataset(window, overlap, decimation_factor = 0):\n","  # Create the input and target data from dataset,\n","  # according to window and overlap\n","  # new dataset 4 dec 2021\n","  # 15 N, 20 AD (resulting indexes: N = 0..14, AD = 15..34)\n","  #Common signals: ['EEG Fp1', 'EEG Fp2', 'EEG F7', 'EEG F3', 'EEG F4', 'EEG F8', 'EEG T3', 'EEG C3', 'EEG C4', 'EEG T4', 'EEG T5', 'EEG P3', 'EEG P4', 'EEG T6', 'EEG O1', 'EEG O2']\n","\n","  tf.random.set_seed(42)\n","  np.random.seed(42)\n","  dataset_dir = working_dir + '/eeg2'\n","  subj_list = tuple((f'{i:02d}', 'N') for i in range(1, 16)) + tuple((f'{i:02d}', 'AD') for i in range(1, 21))\n","  print(subj_list)\n","  num_columns = 16\n","\n","  x_data = np.empty((0, window, num_columns))\n","  y_data = np.empty((0, 1))  # labels\n","  subj_inputs = []  # number of inputs for every subject\n","  print('\\n### creating dataset')\n","  tot_rows = 0\n","  for subject in subj_list:\n","    subj_inputs.append(0)\n","    category = ('N', 'AD').index(subject[1])\n","    eeg = np.load(f'{dataset_dir}/S{subject[0]}_{subject[1]}.npz')['eeg'].T\n","    # SOLUZIONE 2:\n","    # Applicare direttamente alla variabile eeg la funzione ICA e sovrascriverla\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html\n","    #\n","\n","    transformer= sklearn.decomposition.FastICA(n_components=None,  algorithm='parallel',\n","                                               whiten='arbitrary-variance', fun='cube',\n","                                               fun_args=None, max_iter=1000, tol=0.0001,\n","                                               w_init=None, whiten_solver='eigh',\n","                                               random_state=None)\n","    eeg= transformer.fit_transform(eeg)\n","\n","    #\n","    if spikes: eeg = set_holes(eeg, spikes)\n","    #scaler = MinMaxScaler(feature_range=(-1, 1))\n","    scaler = StandardScaler()\n","    eeg = scaler.fit_transform(eeg)\n","    assert(eeg.shape[1] == num_columns)\n","    tot_rows += len(eeg)\n","    # decimation (optional)\n","    if decimation_factor:\n","      eeg2 = np.empty((eeg.shape[0] // decimation_factor, eeg.shape[1]))\n","      for col in range(0, num_columns):\n","        #tmp = scipy.signal.decimate(fusion[:, col], decimation_factor)\n","        tmp = eeg[:, col][::decimation_factor]  # simpler method\n","        eeg2[:, col] = tmp[:len(eeg2)]\n","      eeg = eeg2\n","    # windowing\n","    # compute number of windows (lazy way)\n","    i = 0\n","    num_w = 0\n","    while i + window  <= len(eeg):\n","      i += (window - overlap)\n","      num_w += 1\n","    # compute actual windows\n","    x_data_part = np.empty((num_w, window, num_columns))  # preallocate\n","    i = 0\n","    for w in range(0, num_w):\n","      x_data_part[w] = eeg[i:i + window]\n","      i += (window - overlap)\n","      if False: # watermark provenience of every window\n","        for cc in range(0, num_columns):\n","          x_data_part[w, 0, cc] = 1000 * (len(subj_inputs) - 1) + cc\n","    x_data = np.vstack((x_data, x_data_part))\n","    y_data = np.vstack((y_data, np.full((num_w, 1), category)))\n","    subj_inputs[-1] += num_w\n","\n","  print('\\ntot samples:', tot_rows)\n","  print('x_data:', x_data.shape)\n","  print('y_data:', y_data.shape)\n","  print('windows per subject:', subj_inputs)\n","  print('class distribution:', [np.sum(y_data == cl) for cl in range(0, num_classes)])\n","\n","  return x_data, y_data, subj_inputs"],"id":"20c9e350"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f38b4ecc"},"outputs":[],"source":["def set_holes(A, prob):\n","  for i in range(0, len(A)):\n","    if np.random.rand() < prob:\n","      l = 20\n","      l = int(np.random.normal(l, l / 2))\n","      A[i:i+l,:] = 0\n","  return A"],"id":"f38b4ecc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"664a517b"},"outputs":[],"source":["def partition_data(subjects):\n","  # subjects = tuple (0-based)\n","  x_part = None\n","  y_part = None\n","  subj_inputs_part = []\n","  for subj in subjects:\n","    subj_inputs_part.append(subj_inputs[subj])\n","    skip = sum(subj_inputs[:subj])\n","    num = subj_inputs[subj]\n","    xx = x_data[skip : skip + num]\n","    yy = y_data[skip : skip + num]\n","    if x_part is None:\n","      x_part = xx.copy()\n","      y_part = yy.copy()\n","    else:\n","      x_part = np.vstack((x_part, xx))  # vstack creates a copy of the data\n","      y_part = np.vstack((y_part, yy))\n","  return x_part, y_part, subj_inputs_part"],"id":"664a517b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"68f4d96f"},"outputs":[],"source":["def oversampling(x_data, y_data):\n","  # Duplicate inputs with classes occurring less, so to have a more balanced distribution.\n","  # It operates on single data windows, so use it on data that have already been split\n","  #  by subject (typically only on training data).\n","  x_data_over = x_data.copy()\n","  y_data_over = y_data.copy()\n","  occurr = [np.sum(y_data == cl) for cl in range(0, num_classes)]\n","  for cl in range(0, num_classes):\n","    if occurr[cl] == max(occurr):\n","      continue\n","    mask = y_data[:, 0] == cl\n","    x_dup = x_data[mask].copy()\n","    y_dup = y_data[mask].copy()\n","    while occurr[cl] < max(occurr):\n","      x_dup_jitter = x_dup + np.random.normal(scale=0.03, size=x_dup.shape)\n","      how_many = min(len(y_dup), max(occurr) - occurr[cl])\n","      x_data_over = np.vstack((x_data_over, x_dup_jitter[:how_many]))\n","      y_data_over = np.vstack((y_data_over, y_dup[:how_many]))\n","      occurr[cl] += how_many\n","  return x_data_over, y_data_over"],"id":"68f4d96f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"79984324"},"outputs":[],"source":["def create_model(sample_window, dense1, lstm1, lstm2, lstm3 = 0, dropout = 0.2):\n","  print(\"\\n### creating model\")\n","  model = keras.Sequential()\n","  model.add(keras.Input(shape = sample_window.shape, name = 'input'))\n","  if dense1: model.add(Dense(dense1, name = 'dense1'))\n","  #model.add(BatchNormalization(name = 'norm1'))\n","  #model.add(Dropout(dropout, name = 'drop1'))\n","  model.add(LSTM(lstm1, return_sequences = bool(lstm2), name = 'lstm1'))\n","  if lstm2:\n","    model.add(Dropout(dropout, name = 'drop2'))\n","    model.add(LSTM(lstm2, return_sequences = bool(lstm3), name = 'lstm2'))\n","  if lstm3:\n","    model.add(Dropout(dropout, name = 'drop4'))\n","    model.add(LSTM(lstm3, name = 'lstm3'))\n","  model.add(Dropout(dropout, name = 'drop3'))\n","  model.add(Dense(num_classes, name = 'dense2'))\n","  model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n","    optimizer = 'adam', metrics = ['accuracy'])\n","  model.summary()\n","  return model"],"id":"79984324"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9bbc346"},"outputs":[],"source":["def pca_reduction(A, tol, comp = 0):\n","  assert(len(A.shape) == 2)\n","  dmin = min(A.shape)\n","  if rpca:\n","    r = r_pca.R_pca(A, mu = rpca_mu)\n","    print('Auto tol:', 1e-7 * r.frobenius_norm(r.D), 'used tol:', tol)\n","    print('mu', r.mu, 'lambda', r.lmbda)\n","    L, S = r.fit(tol = tol, max_iter = 10, iter_print = 1)\n","    global norm_s\n","    norm_s = np.linalg.norm(S, ord='fro')  # for debug\n","    print('||A,L,S||:', np.linalg.norm(A, ord='fro'), np.linalg.norm(L, ord='fro'), np.linalg.norm(S, ord='fro'))\n","    #np.savez_compressed('rpca.npz', pre = A, post = L)\n","  elif multiscale_pca:\n","    print('MSPCA...')\n","    #ms = mspca.MultiscalePCA()\n","    #L = ms.fit_transform(A, wavelet_func='sym4', threshold=0.1, scale = True )\n","    print('saving MAT file and calling Matlab...')\n","    scipy.io.savemat('mspca.mat', {'A': A}, do_compression = True)\n","    os.system('matlab -batch \"mspca(\\'mspca.mat\\')\"')\n","    L = scipy.io.loadmat('mspca.mat')['L']\n","  else:\n","    L = A\n","  U, lam, V = np.linalg.svd(L, full_matrices = False)  # V is transposed\n","  assert(U.shape == (A.shape[0], dmin) and lam.shape == (dmin,) and V.shape == (dmin, A.shape[1]))\n","  #np.savetxt('singular_values.csv', lam)\n","  lam_trunc = lam[lam > 0.015 * lam[0]]  # magic number\n","  p = comp if comp else len(lam_trunc)\n","  assert(p <= dmin)\n","  print('PCA truncation', dmin, '->', p)\n","  return L, V.T[:,:p]"],"id":"f9bbc346"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dd5c2185"},"outputs":[],"source":["def reduce_matrix(A, V):\n","  # (N, w, 16) → (N, 16, w) → ((N*16), w) → compute V\n","  # (N, 16, w) * V → transpose again last dimensions\n","  B = np.swapaxes(A, 1, 2)  # (N, 16, w)\n","  C = B.reshape((-1, B.shape[2]))  # ((N*16), w)\n","  if V is None:\n","    L, V = pca_reduction(C, 5e-6, comp = 50)\n","  B = C @ V  # ((N*16), p)\n","  B = B.reshape((A.shape[0], A.shape[2], B.shape[1]))  # (N, 16, p)\n","  return np.swapaxes(B, 1, 2), V  # B = (N, p, 16)"],"id":"dd5c2185"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b94d4ba3"},"outputs":[],"source":["def adjust_size(x, y):\n","  # when flattening the data matrix on the first dimension, y must be made compatible\n","  if len(x) == len(y): return y\n","  factor = len(x) // len(y)\n","  ynew = np.empty((len(x), 1))\n","  for i in range(0, len(y)):\n","    ynew[i * factor : (i + 1) * factor] = y[i]\n","  return ynew"],"id":"b94d4ba3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f42bb1fc"},"outputs":[],"source":["def train_session(save_model = False, load_model = None, write_report = True, file_id = '', earlystop = 0, train_split = 0.75):\n","  def write_values():\n","    print(time.strftime('%H:%M:%S'), file = out_f)\n","    print('window', window, 'overlap', overlap, 'decimation', decimation, file = out_f)\n","    print('layers', dense1, lstm1, lstm2, lstm3, file = out_f)\n","    print('oversample', oversample, file = out_f)\n","    print('pca', pca, file = out_f)\n","    print('rpca', rpca, file = out_f)\n","    print('mspca', multiscale_pca, file = out_f)\n","    print('subj_train', permutation, file = out_f)\n","    print('epochs', epochs, file = out_f)\n","    if history is not None:\n","      print('fit_accuracy', [round(x, 4) for x in history.history['accuracy']], file = out_f)\n","    if history is not None and 'val_accuracy' in history.history:\n","      print('fit_val_accuracy', [round(x, 4) for x in history.history['val_accuracy']], file = out_f)\n","    print('subj_test', subjs_test[perm_index] if subjs_test and type(subjs_test[0]) == tuple else subjs_test, file = out_f)\n","    if subjs_test: print('test_accuracy', round(eval_metrics[1], 4), file = out_f)\n","    print('permutation', perm_index + 1, file = out_f)\n","    print('train_size', len(x_data_train), file = out_f)\n","    print('p', 0 if Vpca is None else Vpca.shape[1], file = out_f)\n","    print('spikes', spikes, file = out_f)\n","    print('time_train', time_train, file = out_f)\n","    if subjs_test: print('time_test', time_test, file = out_f)\n","    print(file = out_f)\n","    out_f.flush()\n","\n","  if write_report:\n","    output_file = time.strftime('%Y%m%d-%H%M%S') + file_id + '.txt'\n","    out_f = open(working_dir + '/' + output_file, 'w')\n","  # tensorboard stuff\n","  %rm -rf \"$log_dir_base\"\n","  log_dir = log_dir_base + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n","  for perm_index, permutation in enumerate(subjs_train_perm):\n","    assert(type(permutation) == tuple)\n","    assert(len(permutation) == 2)\n","    assert(type(permutation[0]) == tuple)\n","    assert(type(permutation[1]) == tuple)\n","    assert(type(subjs_test) == tuple)\n","    tf.random.set_seed(42)\n","    np.random.seed(42)\n","    x_data_train, y_data_train, _ = partition_data(permutation[0])  # train subjects\n","    x_data_val, y_data_val, _ = partition_data(permutation[1])  # validation subjects, can be None\n","    if oversample:\n","      print('\\nclass distribution (training subset):', [np.sum(y_data_train == cl) for cl in range(0, num_classes)])\n","      x_data_train, y_data_train = oversampling(x_data_train, y_data_train)\n","      print('After oversampling (training subset):')\n","      print('x_data_train:', x_data_train.shape)\n","      print('y_data_train:', y_data_train.shape)\n","      print('class distribution:', [np.sum(y_data_train == cl) for cl in range(0, num_classes)])\n","    if False:\n","      #for i in range(0, len(x_data_train)):\n","      #  x_data_train[i] = set_holes(x_data_train[i], spikes)\n","      for w in range(0, len(x_data_train)):  # every window\n","        for c in range(0, x_data_train.shape[2]):  # every column\n","          if np.random.rand() < spikes:\n","            t = 1\n","            for r in range(0, x_data_train.shape[1] // 2):\n","              x_data_train[w, r + x_data_train.shape[1] // 4 , c] = t\n","              t *= -1\n","    # if train_split != 0, we ignore the provided x_data_val and split training in training + validation (Laura)\n","    if train_split:\n","      x_data_train, x_data_val, y_data_train, y_data_val = sklearn.model_selection.train_test_split(x_data_train, y_data_train, train_size = train_split, random_state=42, shuffle=True)\n","    #\n","    #np.savez_compressed('x_data_train.npz', x_data_train = x_data_train)\n","    #print('x_data_train saved')\n","    if pca:\n","      print('\\nPerforming (R)(MS)PCA...')\n","      x_data_train, Vpca = reduce_matrix(x_data_train, None)\n","      y_data_train = adjust_size(x_data_train, y_data_train)\n","      if x_data_val is not None:\n","        x_data_val, _ = reduce_matrix(x_data_val, Vpca)\n","        y_data_val = adjust_size(x_data_val, y_data_val)\n","      print('x_data_train:', x_data_train.shape)\n","      print('y_data_train:', y_data_train.shape, [np.sum(y_data_train == cl) for cl in range(0, num_classes)])\n","      if x_data_val is not None:\n","        print('x_data_val:', x_data_val.shape)\n","        print('y_data_val:', y_data_val.shape, [np.sum(y_data_val == cl) for cl in range(0, num_classes)])\n","    else:\n","      Vpca = None\n","    if load_model is None:\n","      model = create_model(x_data_train[0], dense1, lstm1, lstm2, lstm3)\n","      # draw model to PNG\n","      keras.utils.plot_model(model, to_file = working_dir + '/model.pdf', show_shapes = True)\n","      # model training\n","      print(f'\\n### training with {permutation[0]}, {len(x_data_train)} inputs, {len(x_data_val) if x_data_val is not None else 0} validation')\n","      callbacks = [keras.callbacks.TensorBoard(log_dir + f'_{perm_index + 1}', histogram_freq = 1)]\n","      if earlystop:\n","        callbacks.append(keras.callbacks.EarlyStopping('val_accuracy', min_delta = 0.001, patience = earlystop, restore_best_weights = True, verbose = 1))\n","      # train\n","      start_time = time.monotonic()\n","      history = model.fit(x_data_train, y_data_train, epochs = epochs,\n","        validation_data = (x_data_val, y_data_val) if x_data_val is not None else None,\n","        callbacks = callbacks)\n","      time_train = time.monotonic() - start_time\n","    else:\n","      # model must match with dataset parameters\n","      model = keras.models.load_model(load_model)\n","      history = None\n","      time_train = 0\n","\n","    # model test\n","    if subjs_test:\n","      x_data_test, y_data_test, _ = partition_data(subjs_test[perm_index] if type(subjs_test[0]) == tuple else subjs_test)\n","      if pca:\n","        x_data_test, _ = reduce_matrix(x_data_test, Vpca)\n","        y_data_test = adjust_size(x_data_test, y_data_test)\n","      print(f'\\n### testing with {subjs_test[perm_index] if type(subjs_test[0]) == tuple else subjs_test}, {len(x_data_test)} inputs')\n","      start_time = time.monotonic()\n","      eval_metrics = model.evaluate(x_data_test, y_data_test)\n","      time_test = time.monotonic() - start_time\n","    else:\n","      eval_metrics = [0., 0.]\n","    if write_report:\n","      write_values()\n","    if save_model:\n","      # save in both directory and h5 formats (we had problems with both of them sometimes)\n","      model_file_name = f'{working_dir}/{file_id}model_w{window:04d}_o{overlap:03d}_d{decimation:03d}_e{epochs}_t{round(eval_metrics[1] * 10000)}'\n","      #model.save(model_file_name)\n","      model.save(model_file_name + '.h5')\n","  if write_report:\n","    out_f.close()\n","\n","  return model, x_data_test, y_data_test, eval_metrics[1]  # can be needed by other blocks"],"id":"f42bb1fc"},{"cell_type":"markdown","metadata":{"id":"-Hrx_G7zwnK9"},"source":["## main"],"id":"-Hrx_G7zwnK9"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9d12ae65","outputId":"1e606c7d-38c2-4546-f22f-b771246bedbc","executionInfo":{"status":"ok","timestamp":1699035445751,"user_tz":-60,"elapsed":1522564,"user":{"displayName":"Alessia Conti","userId":"02978993272620717539"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(('01', 'N'), ('02', 'N'), ('03', 'N'), ('04', 'N'), ('05', 'N'), ('06', 'N'), ('07', 'N'), ('08', 'N'), ('09', 'N'), ('10', 'N'), ('11', 'N'), ('12', 'N'), ('13', 'N'), ('14', 'N'), ('15', 'N'), ('01', 'AD'), ('02', 'AD'), ('03', 'AD'), ('04', 'AD'), ('05', 'AD'), ('06', 'AD'), ('07', 'AD'), ('08', 'AD'), ('09', 'AD'), ('10', 'AD'), ('11', 'AD'), ('12', 'AD'), ('13', 'AD'), ('14', 'AD'), ('15', 'AD'), ('16', 'AD'), ('17', 'AD'), ('18', 'AD'), ('19', 'AD'), ('20', 'AD'))\n","\n","### creating dataset\n","\n","tot samples: 5954304\n","x_data: (46483, 256, 16)\n","y_data: (46483, 1)\n","windows per subject: [868, 759, 739, 1560, 1139, 910, 1256, 1879, 1562, 1136, 1130, 1087, 1109, 1498, 1285, 1384, 1430, 1440, 1525, 1329, 1491, 1574, 1662, 1347, 1471, 1596, 1143, 1121, 1539, 1436, 1328, 1544, 1154, 1164, 1888]\n","class distribution: [17917, 28566]\n","\n","class distribution (training subset): [16290, 24312]\n","After oversampling (training subset):\n","x_data_train: (48624, 256, 16)\n","y_data_train: (48624, 1)\n","class distribution: [24312, 24312]\n","\n","Performing (R)(MS)PCA...\n","PCA truncation 256 -> 50\n","x_data_train: (36468, 50, 16)\n","y_data_train: (36468, 1) [18191, 18277]\n","x_data_val: (12156, 50, 16)\n","y_data_val: (12156, 1) [6121, 6035]\n","\n","### creating model\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm1 (LSTM)                (None, 50, 8)             800       \n","                                                                 \n"," drop2 (Dropout)             (None, 50, 8)             0         \n","                                                                 \n"," lstm2 (LSTM)                (None, 8)                 544       \n","                                                                 \n"," drop3 (Dropout)             (None, 8)                 0         \n","                                                                 \n"," dense2 (Dense)              (None, 2)                 18        \n","                                                                 \n","=================================================================\n","Total params: 1362 (5.32 KB)\n","Trainable params: 1362 (5.32 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","### training with (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34), 36468 inputs, 12156 validation\n","Epoch 1/20\n","1140/1140 [==============================] - 63s 51ms/step - loss: 0.2741 - accuracy: 0.8782 - val_loss: 0.1194 - val_accuracy: 0.9616\n","Epoch 2/20\n","1140/1140 [==============================] - 58s 51ms/step - loss: 0.1117 - accuracy: 0.9657 - val_loss: 0.0909 - val_accuracy: 0.9705\n","Epoch 3/20\n","1140/1140 [==============================] - 56s 49ms/step - loss: 0.0914 - accuracy: 0.9732 - val_loss: 0.0777 - val_accuracy: 0.9761\n","Epoch 4/20\n","1140/1140 [==============================] - 57s 50ms/step - loss: 0.0804 - accuracy: 0.9764 - val_loss: 0.0699 - val_accuracy: 0.9774\n","Epoch 5/20\n","1140/1140 [==============================] - 58s 51ms/step - loss: 0.0723 - accuracy: 0.9792 - val_loss: 0.0653 - val_accuracy: 0.9795\n","Epoch 6/20\n","1140/1140 [==============================] - 59s 52ms/step - loss: 0.0657 - accuracy: 0.9807 - val_loss: 0.0646 - val_accuracy: 0.9809\n","Epoch 7/20\n","1140/1140 [==============================] - 56s 49ms/step - loss: 0.0617 - accuracy: 0.9820 - val_loss: 0.0583 - val_accuracy: 0.9831\n","Epoch 8/20\n","1140/1140 [==============================] - 58s 51ms/step - loss: 0.0550 - accuracy: 0.9843 - val_loss: 0.0541 - val_accuracy: 0.9832\n","Epoch 9/20\n","1140/1140 [==============================] - 58s 51ms/step - loss: 0.0496 - accuracy: 0.9846 - val_loss: 0.0539 - val_accuracy: 0.9837\n","Epoch 10/20\n","1140/1140 [==============================] - 58s 51ms/step - loss: 0.0451 - accuracy: 0.9857 - val_loss: 0.0512 - val_accuracy: 0.9851\n","Epoch 11/20\n","1140/1140 [==============================] - 66s 58ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.0488 - val_accuracy: 0.9865\n","Epoch 12/20\n","1140/1140 [==============================] - 59s 52ms/step - loss: 0.0403 - accuracy: 0.9877 - val_loss: 0.0465 - val_accuracy: 0.9864\n","Epoch 13/20\n","1140/1140 [==============================] - 60s 52ms/step - loss: 0.0377 - accuracy: 0.9875 - val_loss: 0.0459 - val_accuracy: 0.9859\n","Epoch 14/20\n","1140/1140 [==============================] - 58s 51ms/step - loss: 0.0345 - accuracy: 0.9894 - val_loss: 0.0456 - val_accuracy: 0.9859\n","Epoch 15/20\n","1140/1140 [==============================] - 61s 54ms/step - loss: 0.0330 - accuracy: 0.9889 - val_loss: 0.0440 - val_accuracy: 0.9873\n","Epoch 16/20\n","1140/1140 [==============================] - 59s 51ms/step - loss: 0.0325 - accuracy: 0.9896 - val_loss: 0.0451 - val_accuracy: 0.9865\n","Epoch 17/20\n","1140/1140 [==============================] - 58s 51ms/step - loss: 0.0300 - accuracy: 0.9903 - val_loss: 0.0410 - val_accuracy: 0.9888\n","Epoch 18/20\n","1140/1140 [==============================] - 59s 52ms/step - loss: 0.0283 - accuracy: 0.9903 - val_loss: 0.0410 - val_accuracy: 0.9878\n","Epoch 19/20\n","1140/1140 [==============================] - 55s 48ms/step - loss: 0.0276 - accuracy: 0.9911 - val_loss: 0.0448 - val_accuracy: 0.9870\n","Epoch 20/20\n","1140/1140 [==============================] - 52s 46ms/step - loss: 0.0264 - accuracy: 0.9915 - val_loss: 0.0441 - val_accuracy: 0.9882\n","\n","### testing with (0, 1, 15, 16, 17), 5881 inputs\n","184/184 [==============================] - 2s 9ms/step - loss: 2.4303 - accuracy: 0.5497\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["# create dataset, create model, train and test\n","if __name__ == '__main__':\n","  dense1 = 0\n","  lstm1 = 8\n","  lstm2 = 8\n","  lstm3 = 0\n","\n","  window = 256\n","  overlap = window // 2\n","  oversample = True\n","  decimation = 0\n","  pca = True   # compute PCA on full data matrix\n","  rpca = False  # compute RPCA before PCA\n","  spikes = 0 # 1/500\n","  rpca_mu = 0.1\n","\n","  subjs_train_perm = ( (tuple(i for i in range(2, 15)) + tuple(i for i in range(18, 35)), ()), )\n","  subjs_test = (0, 1, 15, 16, 17)  # 2 for N, 3 for AD\n","  epochs = 20\n","\n","  if decimation:\n","    window //= decimation\n","    overlap //= decimation\n","\n","  x_data, y_data, subj_inputs = create_dataset(window, overlap, decimation)\n","\n","  model, x_data_test, y_data_test, test_acc = train_session(save_model = True, earlystop = 0)  # returned variables can be optionally used by other blocks of code"],"id":"9d12ae65"},{"cell_type":"markdown","metadata":{"id":"amP9mRRasHd3"},"source":["**Test di un modello già trainato e precaricato**"],"id":"amP9mRRasHd3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a85c45a4"},"outputs":[],"source":["# model test only\n","if __name__ == '__main__':\n","  print(f'### testing with {len(x_data_test)} inputs')\n","  eval_metrics = model.evaluate(x_data_test, y_data_test)"],"id":"a85c45a4"},{"cell_type":"markdown","metadata":{"id":"7jlaze7lsMv0"},"source":["**Calcolo della matrice di confusione**"],"id":"7jlaze7lsMv0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d84905d4"},"outputs":[],"source":["# create confusion matrix\n","if __name__ == '__main__':\n","  import pandas as pd\n","  import seaborn\n","  y_pred = np.argmax(model.predict(x_data_test), axis=-1)\n","  con_mat = tf.math.confusion_matrix(labels = y_data_test, predictions = y_pred).numpy()\n","  con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis = 1)[:, np.newaxis], decimals = 2)\n","  classes = ['N', 'AD']\n","  con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)\n","  figure = plt.figure(figsize = (5, 5))\n","  seaborn.heatmap(con_mat_df, annot = True, cmap = plt.cm.Blues)\n","  plt.tight_layout()\n","  plt.ylabel('True label')\n","  plt.xlabel('Predicted label')\n","  plt.tight_layout()\n","  plt.savefig(working_dir + '/confusion_matrix.png', format='png')\n","  plt.show()"],"id":"d84905d4"}],"metadata":{"colab":{"provenance":[{"file_id":"1U36z8kLQaJb7Rlnh0GaaJ_i3rX-BRGp_","timestamp":1685097234495}]},"gpuClass":"standard","jupytext":{"cell_metadata_filter":"-all","encoding":"# -*- coding: utf-8 -*-","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}